{"id":"sermon-1a3","title":"MVP Infrastructure as Code (Bicep)","description":"## Overview\n\nComplete Bicep IaC to provision the entire MVP. One `az deployment group create` stands up everything for upload → transcribe → score → display.\n\n## What Already Exists (from POCs)\n\n| Resource | Name | Notes |\n|----------|------|-------|\n| Resource Group | rg-sermon-rating-dev | eastus2 |\n| AI Speech | psr-speech-dev | S0, fast transcription proven in POC #6 |\n| Azure OpenAI | psr-openai-dev | Deployments: o4-mini, gpt-41, gpt-41-mini, gpt-4o |\n\nReference as `existing` in Bicep (not recreated).\n\n## What Bicep Must Create\n\n### 1. Storage Account (Blob)\n- Name: psrstoragedev (globally unique)\n- SKU: Standard_LRS, Kind: StorageV2\n- Container: sermon-audio (private)\n- Purpose: uploaded audio files\n\n### 2. Cosmos DB (Serverless)\n- Account: psr-cosmos-dev, API: NoSQL\n- Capacity: Serverless ($0 at idle)\n- Database: psr\n- Container: sermons (partition key: /id) — metadata, transcript, PSR scores, status\n\n### 3. Key Vault\n- Name: psr-kv-dev, SKU: Standard\n- Secrets (populated post-deploy): speech-key, openai-key, openai-endpoint, speech-endpoint, cosmos-connection-string, storage-connection-string\n- Access: RBAC via managed identity\n\n### 4. Log Analytics Workspace\n- Name: psr-logs-dev, SKU: PerGB2018, Retention: 30 days\n- Required dependency for App Insights\n\n### 5. Application Insights\n- Name: psr-insights-dev, Kind: web\n- Linked to Log Analytics workspace\n\n### 6. Function App (Python, Consumption)\n- Name: psr-functions-dev\n- Runtime: Python 3.12, OS: Linux, Plan: Consumption (Y1)\n- App Settings via Key Vault references: SPEECH_KEY, SPEECH_ENDPOINT, OPENAI_KEY, OPENAI_ENDPOINT, COSMOS_CONNECTION_STRING, STORAGE_CONNECTION_STRING\n- AzureWebJobsStorage: direct connection (runtime needs it at startup)\n- Extensions: Durable Functions (azure-functions-durable)\n- Managed Identity: System-assigned\n\n### 7. Static Web App\n- Name: psr-web-dev, SKU: Free\n- Build: Next.js. Bicep creates resource; GitHub Actions deploys code.\n\n### 8. RBAC Assignments (Function App managed identity)\n- Key Vault Secrets User on psr-kv-dev\n- Storage Blob Data Contributor on storage account\n- Cognitive Services User on psr-openai-dev + psr-speech-dev\n- Cosmos DB data-plane contributor\n\n## Bicep File Structure\n\ninfra/\n  main.bicep           — orchestrator, calls all modules\n  main.bicepparam      — dev environment parameter values\n  modules/\n    storage.bicep      — storage account + blob container\n    cosmos.bicep       — Cosmos DB serverless + database + container\n    keyvault.bicep     — Key Vault\n    monitoring.bicep   — Log Analytics + App Insights\n    functions.bicep    — Function App + consumption plan + settings\n    staticwebapp.bicep — Static Web App\n    rbac.bicep         — all managed identity role assignments\n  deploy.sh            — one-liner deploy script\n\n## Deploy\n\naz deployment group create --resource-group rg-sermon-rating-dev --template-file infra/main.bicep --parameters infra/main.bicepparam\n\n## Post-Deploy Steps\n\n1. Populate Key Vault secrets with existing Speech/OpenAI keys\n2. Link Static Web App to GitHub repo\n3. Deploy Function App code (func azure functionapp publish psr-functions-dev)\n\n## Cost at Idle: ~$0.51/mo\n\nStorage ~$0.50, everything else $0 (serverless/free tiers). Per-sermon: ~$0.75.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-28T23:23:49.865465031Z","created_by":"tylerbtt","updated_at":"2026-02-28T23:23:49.865465031Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"sermon-1q2","title":"Request Azure OpenAI quota for GPT-5-mini, GPT-5-nano, GPT-4.1-nano","description":"## Quota Request\n\nSubmit form at https://aka.ms/oai/stuquotarequest\n\n- Subscription: 80b31d19-b663-4936-b8ee-93f7af5b1d27\n- Resource: psr-openai-dev\n- Region: East US 2\n\n### Models to Request\n\n| Model | Deployment Type | Requested TPM |\n|---|---|---|\n| gpt-5-mini | GlobalStandard | 50K |\n| gpt-5-nano | GlobalStandard | 50K |\n| gpt-4.1-nano | GlobalStandard | 50K |\n\n### Current Deployments (working, no changes needed)\n- o4-mini (Pass 1: Biblical Analysis) — KEEP, best reasoning model available\n- gpt-41 / GPT-4.1 (Pass 2: Structure & Content)\n- gpt-41-mini / GPT-4.1-mini (Pass 3: Delivery + Classification)\n\n### Upgrade Path Once Quota Approved\n- gpt-5-mini replaces GPT-4.1 for Pass 2 — better benchmarks, 75% cheaper ($0.25/$2.00 vs $2.00/$8.00)\n- gpt-5-nano replaces GPT-4.1-mini for Pass 3 + classification — 85% cheaper ($0.05/$0.40 vs $0.40/$1.60)\n- gpt-4.1-nano is fallback for classification if GPT-5 quota takes a while\n\n### Cost Impact\nCurrent: ~$0.086/sermon → After upgrade: ~$0.040/sermon (-53%)\n\n### Notes\n- GPT-5 quota requests may take days to weeks per Microsoft forums\n- o4-mini stays for Pass 1 — purpose-built for reasoning, proven in POCs\n- Nothing is blocked; current models are solid","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-28T19:23:34.713184361Z","created_by":"tylerbtt","updated_at":"2026-02-28T19:23:45.019592655Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"sermon-1qd","title":"API Layer (Azure Functions HTTP triggers)","description":"Three HTTP endpoints the frontend needs, running as Azure Functions:\n\n## POST /api/sermons — Upload\n\nAccept audio upload, validate, kick off processing.\n\nValidation:\n- File type: MP3, WAV, M4A only (check Content-Type + magic bytes)\n- File size: max 100MB (Azure Functions consumption plan request limit)\n- WAV files over 100MB: reject with error suggesting MP3 conversion. Fast transcription API accepts MP3 directly — no need to convert to WAV.\n- Duration check: not possible pre-upload without processing. WPM sanity check catches bad files post-transcription.\n\nFlow:\n1. Validate file\n2. Create sermon record in Cosmos DB (status: processing, title: filename, uploadedAt: now)\n3. Store audio in Blob Storage (sermon-audio container, key: {id}/{original_filename})\n4. Start Durable Functions orchestrator (pass sermon ID + blob URL)\n5. Return sermon ID immediately\n\nResponse: {id, status: processing}\n\n## GET /api/sermons — Feed List\n\nReturn all sermons for the feed page. No pagination for MVP.\n\nResponse: [{id, title, pastor, date, duration, status, sermonType, compositePsr}]\n- compositePsr is null when status is processing\n- title/pastor come from LLM metadata extraction (step 4 of pipeline)\n- If still processing, title defaults to original filename\n\n## GET /api/sermons/{id} — Full Detail\n\nReturn full sermon document from Cosmos DB. Frontend reads: metadata, all 8 category scores + weights + reasoning, composite, strengths/improvements, transcript with segment classifications, radar chart data.\n\n## CORS\n\nConfigure for Static Web App origin. In dev: localhost:3000.\n\n## Data Contracts\n\nDefined in docs/frontend-spec.md. The API returns Cosmos DB documents directly — no transformation layer for MVP.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-03-01T10:16:57.591715036Z","created_by":"tylerbtt","updated_at":"2026-03-01T10:48:18.539814403Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"sermon-1qd","depends_on_id":"sermon-1a3","type":"blocks","created_at":"2026-03-01T10:17:01.318510466Z","created_by":"tylerbtt","metadata":"{}","thread_id":""},{"issue_id":"sermon-1qd","depends_on_id":"sermon-1rn","type":"blocks","created_at":"2026-03-01T10:17:01.418913089Z","created_by":"tylerbtt","metadata":"{}","thread_id":""}]}
{"id":"sermon-1rn","title":"Sermon Processing Pipeline (Durable Functions)","description":"Durable Functions orchestrator that takes uploaded audio through the full pipeline:\n\n1. Transcribe via Azure AI Speech fast transcription API\n2. Audio analysis via Parselmouth (pitch, volume, pauses)\n3. Fan-out 3 parallel GPT scoring passes:\n   - Pass 1: Biblical Analysis (o4-mini) — Biblical Accuracy, Time in the Word, Passage Focus\n   - Pass 2: Structure & Content (GPT-4.1) — Clarity, Application, Engagement\n   - Pass 3: Delivery (GPT-4.1-mini) — Delivery, Emotional Range\n4. Fan-in + sermon type classification + metadata extraction (GPT-4.1-mini)\n   - Classify: expository / topical / survey\n   - Extract: sermon title, pastor name, main passage, date/occasion if mentioned\n5. Segment classification (GPT-4.1-mini) — label transcript segments as scripture/teaching/application/anecdote/illustration/prayer/transition. Required for frontend transcript viewer color-coding.\n6. Score normalization by sermon type (pure code)\n7. Store results in Cosmos DB, update status to complete\n\n## Parselmouth Hosting\n\nParselmouth is a single 36MB .so with standard libc deps (no exotic native binaries). It CAN run on Azure Functions Linux consumption plan — the sandbox supports standard C++ libs. The 36MB adds to cold start but is within the 1.5GB memory limit. For MVP, deploy Parselmouth as part of the Function App package. If cold starts become a problem, move to a dedicated activity function on Premium plan later.\n\n## Error/Retry Strategy\n\nDurable Functions built-in retry policies:\n- Transcription (fast API): retry 2x, 30s backoff. If permanent fail → status: failed, error: transcription_error\n- Parselmouth audio analysis: retry 1x. If fail → proceed WITHOUT audio metrics, flag delivery/emotional_range scores as partial (audio_metrics: null). Graceful degradation.\n- Each LLM pass (1-3): retry 2x, 10s backoff. If one pass permanently fails → status: failed, error: scoring_error, include which pass failed\n- Classification + metadata: retry 2x, 10s backoff. If fail → default to type: unknown, title: filename, pastor: unknown\n- Segment classification: retry 1x. If fail → store transcript without segment labels, frontend shows unsegmented text\n- Overall orchestrator timeout: 10 minutes\n- On any permanent failure: update Cosmos DB status to failed with error message and failed_at timestamp\n\n## WPM Sanity Check\n\nAfter transcription, check WPM. If < 80 or > 200, flag as suspect but continue processing. Store wpm_flag: true in results so frontend can show a warning.\n\n## Depends On\n\nInfra (sermon-1a3) being deployed. POC #7 validates the full pipeline produces accurate scores on complete transcripts.\n\n## Cost Target\n\n~$0.75 per sermon ($0.67 speech + $0.09 OpenAI). POC #7 confirmed $0.09 OpenAI on 3,762-word transcript.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-03-01T10:16:50.743554039Z","created_by":"tylerbtt","updated_at":"2026-03-01T10:48:01.489137756Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"sermon-1rn","depends_on_id":"sermon-1a3","type":"blocks","created_at":"2026-03-01T10:17:01.207969538Z","created_by":"tylerbtt","metadata":"{}","thread_id":""}]}
{"id":"sermon-32a","title":"POC #7: Validate multipass scoring on full transcript","description":"Re-run POC #5 multipass scoring on POC #6's complete transcript (3,762 words vs 1,064). Validates that scores are accurate on full text. Result: composite 88.0 vs 82.7 — partial transcript underestimated by 5.3 points. Application +13, Clarity +10, Delivery +10. Scripture refs: 12 vs 6. See poc/validated_multipass_poc.py and docs/research.md POC #7 section.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-03-01T10:49:19.930097931Z","created_by":"tylerbtt","updated_at":"2026-03-01T10:49:20.180087670Z","closed_at":"2026-03-01T10:49:20.180067894Z","close_reason":"Completed. Composite PSR 88.0 on full transcript (vs 82.7 on partial). All findings documented in docs/research.md and poc/README.md.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"sermon-36d","title":"Build MVP Frontend","description":"Full spec at docs/frontend-spec.md. Three pages:\n\n1. Upload page (/) — drag-and-drop audio file, redirect to detail page\n2. Sermon feed (/sermons) — ESPN QBR-style sortable table\n3. Sermon detail (/sermons/{id}) — Lighthouse-style scorecard with radar chart\n\nTech: Next.js + TypeScript + Tailwind CSS + Recharts\nDeploy: Azure Static Web Apps (free tier)\n\nKey Design Decisions:\n- Lighthouse report pattern for the detail page (big score, category grid, expandable reasoning)\n- Opta-style 8-axis radar chart as secondary visual\n- Three score colors only: green (70+), yellow (50-69), red (below 50)\n- Light theme only for MVP\n- No component library — Tailwind utilities only\n- Poll for processing status (no websockets)\n\nAPI Contracts:\n- POST /api/sermons (upload)\n- GET /api/sermons (feed list)\n- GET /api/sermons/{id} (full detail)\n\nSee docs/frontend-spec.md for wireframes, data contracts, component inventory, and design tokens.\nUI inspiration research in bead sermon-3bz.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-02-28T20:32:09.575828221Z","created_by":"tylerbtt","updated_at":"2026-03-01T10:17:04.629179712Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"sermon-36d","depends_on_id":"sermon-1qd","type":"blocks","created_at":"2026-03-01T10:17:04.629120162Z","created_by":"tylerbtt","metadata":"{}","thread_id":""}]}
{"id":"sermon-3bz","title":"UI Design Research and Inspiration","description":"## UI Design Research — Inspiration Sources and Patterns\n\n### Design Philosophy\nMinimalist. No AI slop. Let the data speak. Every element earns its place.\n\n---\n\n## Source 1: ESPN QBR Leaderboard\nURL: espn.com/nfl/qbr\n\nKey patterns:\n- Clean sortable table: rank, name, QBR score, then sub-columns (PAA, PLAYS, EPA, PASS, RUN, SACK, PEN, RAW)\n- The composite number is the hero — big, bold, left-aligned. Sub-metrics are secondary, smaller\n- Season/week toggle tabs at top — maps to our sermon feed filtering\n- Conference/division filter dropdown — maps to sermon type filter (expository/topical/survey)\n- No color gradients or gauges — just numbers in a table. Trusts the reader to interpret\n- Minimal chrome: white background, thin borders, team abbreviation badges\n\nWhat to steal for PSR:\n- Sermon feed page as a sortable table: PSR score prominent, category scores as secondary columns\n- Filter by sermon type, date range\n- Click a row to go to sermon detail page (like clicking a player goes to player page)\n\n---\n\n## Source 2: Opta Player Radars (Pizza Charts)\nURL: theanalyst.com/articles/introducing-opta-radars-compare-players\n\nKey patterns:\n- Polar area chart (pizza slices) — each slice is one metric, slice length is percentile rank\n- 9 metrics grouped into 3 themes (attacking, possession, defending) with color coding per group\n- Player info (name, team, position, season) in top-left corner\n- Raw values AND percentile shown per slice on hover/detail view\n- Comparison mode: two players side-by-side with overlapping radars\n- The shape tells the story — a well-rounded player has a full circle, a specialist has spikes\n\nWhat to steal for PSR:\n- 8-slice radar for the sermon detail page — one slice per PSR category\n- Group slices by color: blue for biblical (accuracy, time in word, passage focus), green for content (clarity, application, engagement), orange for delivery (delivery, emotional range)\n- The shape instantly communicates sermon style — heavy biblical focus vs balanced vs delivery-heavy\n- Future: side-by-side comparison of two sermons from same pastor\n\nWhy radar over bar chart: With 8 categories, a bar chart is boring. The radar shape becomes a visual fingerprint — you start recognizing patterns. That is a typical Piper shape — huge biblical spike, moderate application.\n\n---\n\n## Source 3: Rotten Tomatoes Score Box\nURL: rottentomatoes.com (score box redesign 2021)\n\nKey patterns:\n- Two scores side by side: Tomatometer (critics) plus Audience Score (users) — each with its own icon\n- Score plus icon plus count of reviews underneath\n- Critics Consensus — one-sentence summary below the scores\n- Audience Says — crowd-sourced sentiment summary\n- Click score to drill into individual reviews\n- Traffic light iconography: fresh tomato (good), splat (bad), certified fresh (great)\n- Metadata row above scores: year, genre, runtime, rating\n\nWhat to steal for PSR:\n- The composite PSR score as the hero number with a simple icon/badge system:\n  - 90-100: gold/elite badge\n  - 70-89: green/solid\n  - 50-69: yellow/average\n  - Below 50: red/needs work\n- One-sentence AI-generated summary below the score (like Critics Consensus): Strong expository sermon with excellent biblical grounding. Application could be more concrete.\n- Strengths plus improvements as bullet points (already in our data model)\n- Sermon metadata row: pastor, church, date, duration, sermon type badge\n\n---\n\n## Source 4: Google Lighthouse\nClosest analog to PSR scoring model.\n\nKey patterns:\n- Circular gauge per category (Performance, Accessibility, Best Practices, SEO) — color-coded green/orange/red\n- Each gauge is a simple arc, not a full circle — clean and readable\n- Weighted composite explained: hover to see Performance is 25 percent of overall score\n- Expandable detail sections below each category with specific findings\n- Traffic light colors: 0-49 red, 50-89 orange, 90-100 green\n\nWhat to steal for PSR:\n- 8 categories, weighted composite — almost identical model\n- Sermon detail page: composite gauge at top, then 8 smaller category gauges in a grid below\n- Each category gauge is clickable — expands to show the LLM reasoning/evidence\n- Color coding: same traffic light (red/yellow/green) based on score\n- Weight labels on each category: Biblical Accuracy (25 percent)\n\nWhy this works: Lighthouse is universally understood. The pattern is proven for here is your overall score, here is why. PSR is Lighthouse for sermons.\n\n---\n\n## Source 5: Chronicle.church\nURL: chronicle.church\n\nKey patterns:\n- Sermon card in library: title, pastor name, date, duration, status badge (Completed)\n- Clean list layout — no grid, no thumbnails. Text-forward\n- Detail page tabs: Full Analysis, Repurpose Content, Discussion Questions, Personal Notes\n- AI analysis presented as structured sections: Themes, Key Points, Verses, Questions, Quick Summary\n- Upload flow: record in-app or upload file. Support MP3, WAV, M4A\n\nWhat to steal for PSR:\n- Sermon card design: title, pastor, date, duration — but ADD the PSR score badge prominently\n- The card list is the right pattern for a sermon feed (not a grid of thumbnails)\n- Tab structure on detail page could work: Scorecard | Transcript | Analysis\n\nWhat to avoid from Chronicle:\n- Landing page is heavy on marketing copy and feature lists — feels like SaaS sales\n- Too many feature callouts. PSR should lead with the score, not the features\n- Chronicle does NOT score sermons — scoring is our differentiator\n\n---\n\n## Source 6: Minimal Podcast App\nURL: minimalpodcast.app\n\nKey patterns:\n- Anti-algorithm, anti-noise philosophy\n- Signal vs Noise framing: Your Subscriptions plus Chronological Order plus Pure Audio Focus\n- Monochrome palette with one accent color\n- Massive whitespace. Typography does the heavy lifting\n- No feature grid. No screenshot carousel. Just the app and a sentence\n\nWhat to steal for PSR:\n- The anti-slop philosophy. No gradient backgrounds, no floating particles, no AI-powered badges everywhere\n- Let the score be the interface. Upload, wait, score. That is it\n- Generous whitespace on the detail page. Do not cram 8 category scores into a tiny card\n- Dark mode as default or option — sermon content feels more focused on dark backgrounds\n- Typography-first design: the numbers and text ARE the design\n\n---\n\n## Source 7: QB Throwing Scorecards (The Spade / Substack)\nURL: thespade.substack.com\n\nKey patterns:\n- Self-contained card with all key stats: donut chart (completions/attempts), rolling EPA graph, pass contour map\n- Team-colored accent throughout the card\n- Designed to be screenshot-able and shareable on social media\n- Multiple data visualizations in one card without feeling cluttered\n\nWhat to steal for PSR:\n- A shareable sermon scorecard image/card for social media\n- Contains: PSR score, radar chart, pastor name, sermon title, date, top 3 category scores\n- Designed as a fixed-size card (1080x1080 or 1200x628) for social sharing\n- Phase 1 feature but worth designing the detail page with shareability in mind\n\n---\n\n## Source 8: Credit Score Displays (Credit Karma, Experian)\n\nKey patterns:\n- Large circular gauge with the score number centered inside\n- Color gradient around the arc (red to yellow to green)\n- Below the gauge: Factors affecting your score as a list with impact indicators (high/medium/low)\n- Each factor is expandable for detail\n- Historical trend line showing score over time\n\nWhat to steal for PSR:\n- The composite PSR gauge: large circle, score number centered, color-coded arc\n- Below: 8 categories listed with horizontal bar showing score, sorted by weight\n- Each category expandable to show LLM reasoning\n- Future: pastor trend line showing PSR over time (Phase 1)\n\n---\n\n## Proposed PSR Page Layouts\n\n### Upload Page\n- Dead simple. Centered on screen\n- Drag-and-drop zone or file picker button\n- Accepted formats note (MP3, WAV, M4A)\n- Max duration note (1 hour)\n- No form fields in MVP (no title, no pastor name — extract from content or add later)\n- After upload: redirect to detail page showing processing status\n\n### Sermon Feed (Home)\n- ESPN QBR table style: sortable list\n- Columns: PSR Score (hero), Sermon Title, Pastor (if known), Date, Duration, Type badge\n- Click row to go to detail page\n- Filter: sermon type (all/expository/topical/survey), sort by score/date\n- No pagination in MVP — just show all (will be under 50 sermons)\n\n### Sermon Detail Page\nLayout (top to bottom):\n1. Header: sermon title, pastor, date, duration, type badge\n2. Score Hero: large circular gauge with composite PSR (Lighthouse/credit score style)\n3. One-line summary: AI-generated (like RT Critics Consensus)\n4. Category Breakdown: 8 categories in a 2x4 grid, each with category name plus weight label, horizontal bar or small gauge showing score, color coded (green/yellow/red), click to expand for LLM reasoning\n5. Radar Chart: Opta-style 8-axis radar showing the sermon shape\n6. Strengths and Improvements: bullet points (already in data model)\n7. Transcript: scrollable text with segment type color coding (scripture=blue, teaching=gray, application=green, anecdote=orange)\n\n### Design Tokens\n- Font: Inter or system font stack (no custom fonts in MVP)\n- Colors: neutral base (gray-50 to gray-900), accent for score tiers (green-500, yellow-500, red-500)\n- Spacing: generous — 16px minimum between elements\n- Border radius: subtle (4-8px), not pill-shaped\n- No shadows except subtle card elevation\n- Dark mode: consider as default for the detail page\n\n---\n\n## Anti-Patterns (What NOT to Do)\n- No gradient mesh backgrounds\n- No floating 3D elements or particles\n- No Powered by AI badges\n- No feature comparison grids on the landing page\n- No testimonial carousels\n- No chat widgets\n- No skeleton loaders that look like the actual content (use a simple spinner or progress bar)\n- No confetti animations when a score is revealed\n- No gamification (streaks, badges, achievements) in MVP","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-28T20:19:51.921596780Z","created_by":"tylerbtt","updated_at":"2026-02-28T20:20:16.176600387Z","source_repo":".","compaction_level":0,"original_size":0}
